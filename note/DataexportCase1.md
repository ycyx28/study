# 关于数据导出的case

## 一.case1

### 场景

 将若干个商家的所有交易数据全部导出到excel，历史数据已经迁移到历史库，下单时间按月分区，每个商家数据在100w条以上。
 
> ### 版本1

 按商家分月查寻数据，使用poi解析导出到excel。
 
>> #### 执行效果

执行效率非常慢，执行时间需要12H+，严重影响效率，查询日志发现查询sql速度很快，没有slow sql，但是excel生成却非常慢。

>> #### 问题分析

1. 每个商家数据量大于100w，内存消耗大，也是处理速度慢的一个原因。

2. 一次性保存落地到excel文件，磁盘io达到瓶颈，性能消耗高，导致处理极慢。

3. 导出到excel，需要进行数据解析和格式转换，速度慢的最重要的原因。

> ### 版本2

通过版本1问题分析，迅速修改方案

1. 不使用POI组件，使用原生IO的输入输出留，导出excel格式为.CSV，以逗号分隔，用CSV格式的Excel导出数据节省了额外的数据解析时间；

2. 商家数据按月查询且按月落地保存，每月数据都在上月生产的CSV文件进行追加，这样就解决了100w+的数据放在内存同时落地产生的内存消耗和磁盘IO瓶颈，大大提升了效率。

>> #### 执行效果

查询sql和导出速度非常快，由原来的12H+的时间提升到现在不到30分钟，但是由于使用逗号分隔来生产CSV文件，在生产CSV文件的时候有些小问题。

>> #### 优化方案

逗号用双引号包裹，如果字段中既有逗号，又有双引号，或者字段中有双引号开头，如果数据"aaa"、"aaa,ss"、aaa,ss需要在数据头尾加上双信号，且逗号用双引号包裹，数据中原有双引号需要替换成2个双引号，修改后数据为""aaa""、""aaa","ss""、"aaa","ss"。

问题解决

## 二.case2

### 场景

 将近一年的交易流水详情导出到另一张表A，交易数据由多张表的数据组成，交易流水按交易下单时间按月分区，每天交易数据60w+
 
> ### 版本1

  逐月查询交易数据，每500条分页查询一次数据保存到表A。
  
>> #### 执行效果

每天sql执行时间都超过1分钟，执行速度非常慢。

>> #### 问题分析

每天数据量有60w+，一个月数据量就是1600w+，在1600w+的数据量上再按500条数据分页查询，这个级别的数据量分页查询非常耗数据库性能，至于这个要分析Oracle内部性能（如果要从1600w+的数据中取500条数据，真实情况数据库处理的数据量远不止500，只是最终只拿出500条）。

> ### 版本2

问题找到，优化方案自然就很容易出来，把每次查询改为逐天查询交易数据，每500条分页查询一次数据保存到表A。

>> #### 执行效果

速度有明显提升，几乎没有慢sql，但是每次count条数的时候还是会出现。全部数据导出完后发现数据量少了很多，并且有很多重复数据。

>> #### 问题分析

查询代码和各种测试发现，做分页的时候没有使用Order By 进行排序，分页查询的数据不准确，每次查询的数据不能保证在前几次中唯一。

> ### 版本3

针对版本2中数据混乱问题，我们给加上了Order By，根据主键进行排序，测试环境发现没有任何问题，马上改完就放到线上去执行了。

>> #### 执行效果

已上线发现sql执行非常慢，几乎每条都是slow sql，而且查询时间基本都超过1分钟，一天执行下来处理不到100w条数据。

>> #### 问题分析

因为版本3和版本2相比，只多了order By ，查看资料得知，使用order by非常耗性能，一天的数据量是60w，每次查500条，每次都需要对60w的数据排序后再去对应的500条数据，所以速度慢很容易理解。

> ### 版本4

问题分析完毕，版本4的方案也就很容易出来了，取消分页，将时间查询时间再次缩小进行分片，每小时查询一次数据，逐条保存到数据库，修改程序后测试执行发现速度非常可观。

>> #### 执行效果

 查询和插入速度都非常快，没有出现慢sql，问题解决。











